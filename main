!pip install -U "langchain>=0.1.17" "langchain-community>=0.1.17" \
               "langchain-core>=0.3.6" "langgraph>=0.0.52" \
               "langchain-openai>=0.1.7" openai tiktoken \
               pandas numpy matplotlib seaborn -q

import os, time, random, json, pathlib
import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from langgraph.graph import StateGraph, START, END
from langchain_community.chat_models import ChatOpenAI
from langchain_core.messages import HumanMessage

os.environ["OPENAI_API_KEY"] = "sk-proj-xxxxx"  # your key 
random.seed(42)

N_RUNS = 200
DRIFT_CONF_THRESHOLD = 0.6    # drift sensitivity
REFLEX_WEIGHTS = [
    ("auto-replan", 0.45),
    ("rollback", 0.25),
    ("tool-retry", 0.20),
    ("human-approve", 0.10)
]

CORPUS = [
 ("langgraph","LangGraph enables stateful multi-agent coordination via graph execution, shared state, and deterministic edges."),
 ("autogen","AutoGen supports multi-agent conversations, tool calls, and role hand-offs using message loops."),
 ("crew","CrewAI organizes agents with roles, tasks, and delegation for collaborative problem solving."),
 ("llamaindex","LlamaIndex provides modular retrieval, memory, and routing for LLM applications and agent frameworks."),
 ("openagents","OpenAgents offers an open platform for building agent ecosystems with standardized interfaces."),
 ("guardrails","Guardrails enforce schema, policy, and safety constraints; violations trigger validation or blocklist actions."),
 ("observability","Observability pipelines collect traces, tool outputs, prompts, metrics, and policy decisions for audits."),
 ("rollback","Rollback restores a prior checkpointed state to recover from undesired side-effects or drift."),
 ("sandbox","Sandbox execution isolates risky actions, external APIs, or unknown code before promoting results."),
 ("audit","Audit snapshots capture prompts, tools, decisions, and outputs for forensic analysis and compliance."),
 ("policy","A meta-monitor maps triggers to reflex actions with thresholds, SLAs, and approval requirements."),
 ("mttra","MTTR-A measures recovery latency from fault detection to restored stability in agent workflows."),
 ("mtbf","MTBF is the mean time between cognitive faults or drift events in a running agentic system."),
 ("nrr","NRR = 1 − MTTR-A/MTBF provides a dimensionless reliability score comparing recovery to fault rate."),
 ("drift","Reasoning drift includes plan divergence, wrong context, stale memory, unsafe tool calls, or low confidence."),
 ("rate-limit","Tool timeouts and API rate limits cause transient failures; retries or backoff can mitigate."),
 ("consensus","Consensus or voting aggregates peer agent outcomes to handle disagreement and reduce single-agent error."),
 ("memory","Global memory reconciliation prevents corrupted entries from propagating across agents."),
 ("escalation","Human approval gates high-impact actions; escalations add latency but increase assurance."),
 ("tool-retry","Tool-retry repeats an operation with jitter, backoff, or alternative endpoints to handle transient faults."),
 ("auto-replan","Auto-replan regenerates the plan with simplified steps or safer tools under policy constraints."),
 ("safe-mode","Safe-mode narrows the allowed action set and restricts external side-effects until confidence improves."),
 ("fallback","Fallback routes to cached data, read-only operations, or lower-risk strategies."),
 ("snapshot","Snapshots persist intermediate artifacts, enabling reproducible rollbacks and post-mortems."),
 ("sre","SRE practices like SLOs, MTTR, canarying, and incident playbooks inspire agentic reliability."),
 ("telemetry","Cognitive telemetry exports drift signals, confidence, votes, and tool success rates."),
 ("latency","Detection, decision, and execution latencies compose the total recovery window."),
 ("retrieval","Hybrid retrieval blends keyword, vector, and reranking for robust document grounding."),
 ("summarization","Structured summarization extracts claims, sources, and risk flags from retrieved text."),
 ("planning","Planning decomposes tasks into toolable steps with pre-/post-conditions and checks."),
 ("evaluation","Offline evals replay logs, compute MTTR-A, MTBF, NRR, and compare policies."),
 ("governance","Governance codifies risk tiers, approvals, red-teams, and reporting."),
 ("async","Asynchronous agents coordinate via queues; retries and idempotency matter."),
 ("cache","Response caches reduce tool churn and latency; invalidation prevents stale reads."),
 ("routing","Policy-based routing selects agents or tools by risk, cost, and confidence signals."),
 ("traces","Fine-grained traces capture prompts, tools, and decisions for reproducibility."),
 ("configs","Configuration drift is mitigated with templates, tests, and checksums."),
 ("secrets","Secret handling avoids prompt leakage and records masked artifacts."),
 ("backoff","Jittered exponential backoff smooths contention and API rate-limit spikes."),
 ("timeouts","Budgeted timeouts bound worst-case latencies and trigger safe abort."),
 ("playbooks","Incident playbooks standardize reflexes and escalation paths."),
 ("checkpoints","Checkpoints persist minimal state for deterministic, replayable recovery.")
]

QUERY_POOL = [
  "LangGraph recovery reflexes", "agent orchestration reliability",
  "rollback sandbox audit snapshots", "tool retries and backoff",
  "consensus voting disagreement", "policy thresholds and approvals",
  "governance and risk tiers", "observability telemetry signals",
  "drift detection and confidence", "mttra and mtbf calculation",
  "normalized reliability index", "incident playbooks escalation",
  "global memory reconciliation", "safe mode fallback routes",
  "retrieval reranking grounding", "planning decomposition tools"
]

LOGDIR = pathlib.Path("telemetry"); LOGDIR.mkdir(exist_ok=True)
LOGFILE = LOGDIR / f"langgraph_run_{int(time.time())}.jsonl"

def jlog(**kv):
    kv["ts"] = time.time()
    with open(LOGFILE, "a") as f: f.write(json.dumps(kv) + "\n")

# RETRIEVAL & CONFIDENCE 
def local_search(query: str, k: int = 3):
    time.sleep(0.6 + random.random()*0.4) 
    q = query.lower().split()
    bigrams = set(zip(q, q[1:])) if len(q) > 1 else set()
    scored = []
    for _, txt in CORPUS:
        tl = txt.lower().split()
        score = sum(w in tl for w in q)
        score += sum((a in tl and b in tl) for (a,b) in bigrams)
        scored.append((txt, score, len(txt)))
    scored.sort(key=lambda x: (x[1], x[2]), reverse=True)
    return [s[0] for s in scored[:k]]

def confidence(snippets):
    L = sum(len(s) for s in snippets)
    return max(0.0, min(1.0, (L / 600)))


# LANGGRAPH NODES =====================================

def reasoning_node(state):
    state["t_reason_start"] = time.time()
    query = state["query"]
    snippets = local_search(query)
    state["snippets"] = snippets
    state["confidence"] = confidence(snippets)
    state["t_reason_end"] = time.time()
    jlog(run=state["run_id"], event="reasoning_done", conf=state["confidence"])
    return state

def check_drift_node(state):
    state["t_drift_check"] = time.time()
    state["is_drift"] = (state["confidence"] < DRIFT_CONF_THRESHOLD) or (random.random() < 0.05)
    jlog(run=state["run_id"],
         event="fault_detected" if state["is_drift"] else "no_fault",
         conf=state["confidence"])
    return state

def recovery_node(state):
    state["t_recovery_start"] = time.time()
    if not state["is_drift"]:
        state.update({"recovery_mode": "no-drift", "recovery_delay": 0})
        jlog(run=state["run_id"], event="recovered", mode="no-drift")
        return state

    # ---- Decision latency ----
    t_decide_start = time.time()
    modes, weights = zip(*REFLEX_WEIGHTS)
    mode = random.choices(modes, weights=weights, k=1)[0]
    time.sleep(random.uniform(0.2, 0.6))
    t_decide_end = time.time()

    # ---- Execute reflex ----
    t_exec_start = time.time()
    if mode == "tool-retry":
        time.sleep(random.uniform(3.0, 5.0))
        _ = local_search(state["query"] + " recovery")
    elif mode == "auto-replan":
        time.sleep(random.uniform(4.0, 6.5))
        _ = local_search(state["query"] + " orchestration")
    elif mode == "rollback":
        time.sleep(random.uniform(5.5, 7.0))
        state["snippets"] = []
        _ = local_search(state["query"])
    elif mode == "human-approve":
        time.sleep(random.uniform(10.0, 12.5))
    t_exec_end = time.time()

    # ---- Record metrics ----
    state["recovery_mode"] = mode
    state["T_detect"] = state["t_drift_check"] - state["t_reason_end"]
    state["T_decide"] = t_decide_end - t_decide_start
    state["T_execute"] = t_exec_end - t_exec_start
    state["recovery_delay"] = state["T_decide"] + state["T_execute"]

    jlog(run=state["run_id"], event="reflex_selected", mode=mode)
    jlog(run=state["run_id"], event="recovered", mode=mode,
         T_detect=state["T_detect"], T_decide=state["T_decide"],
         T_execute=state["T_execute"])
    return state

# GRAPH BUILD 
graph = StateGraph(dict)
graph.add_node("reasoning", reasoning_node)
graph.add_node("check_drift", check_drift_node)
graph.add_node("recovery", recovery_node)
graph.add_edge(START, "reasoning")
graph.add_edge("reasoning", "check_drift")
graph.add_edge("check_drift", "recovery")
graph.add_edge("recovery", END)
app = graph.compile()

# RUN EXPERIMENT 
records = []
for i in range(N_RUNS):
    init = {"run_id": i, "query": random.choice(QUERY_POOL)}
    final = app.invoke(init)
    records.append({
        "run_id": i,
        "is_drift": final["is_drift"],
        "recovery_mode": final["recovery_mode"],
        "delay_sec": final.get("recovery_delay", 0),
        "T_detect": final.get("T_detect", 0),
        "T_decide": final.get("T_decide", 0),
        "T_execute": final.get("T_execute", 0),
        "t_recovered": final.get("t_recovery_start", 0)
    })
    if i % 20 == 0:
        print(f" Completed {i}/{N_RUNS} runs...")

df = pd.DataFrame(records)
print("\n Finished all runs.")

# METRICS
valid = df[df.delay_sec > 0]["delay_sec"]
if len(valid) == 0:
    print("No drift/recovery events detected —> increase DRIFT_CONF_THRESHOLD.")
else:
    mttr_a = valid.median(); mttr_std = valid.std()
    p90 = np.percentile(valid, 90)
    drift_rate = df.is_drift.mean()
    total_time = df["t_recovered"].max() - df["t_recovered"].min()
    num_drifts = df["is_drift"].sum()
    mtbf = total_time / num_drifts if num_drifts > 0 else float("inf")
    mtbf_std = np.std(np.diff(df.sort_values("t_recovered")["t_recovered"].values)) if num_drifts > 1 else 0
    nrr = 1 - (mttr_a / mtbf)

    print(f"\nMedian MTTR-A: {mttr_a:.2f} ± {mttr_std:.2f}s | P90: {p90:.2f}s | Drift rate: {drift_rate:.1%}")
    print(f"MTBF ≈ {mtbf:.2f} ± {mtbf_std:.2f}s | NRR ≈ {nrr:.3f}")

    summary = (
        df[df.delay_sec > 0]
        .groupby("recovery_mode")
        .agg(Median_MTTR_A=("delay_sec", "median"),
             Std_MTTR_A=("delay_sec", "std"),
             P90_MTTR_A=("delay_sec", lambda x: np.percentile(x, 90)),
             Count=("run_id", "count"))
        .reset_index()
    )
    print("\nSummary per recovery mode:\n", summary.round(2))

# OPTIONAL PLOTS
plt.figure(figsize=(7,4))
sns.histplot(valid, bins=25, color="royalblue", kde=True)
plt.title("MTTR-A Distribution (LangGraph MAS)")
plt.xlabel("Recovery Time (s)"); plt.ylabel("Runs"); plt.grid(alpha=.3); plt.show()

plt.figure(figsize=(7,4))
sns.boxplot(data=df[df.recovery_mode != "no-drift"], x="recovery_mode", y="delay_sec", palette="Set2")
plt.title("MTTR-A by Recovery Mode"); plt.xlabel("Mode"); plt.ylabel("Delay (s)")
plt.grid(axis="y", alpha=.3); plt.show()

print(f"\n Telemetry log saved to: {LOGFILE}")


df["window_avg"] = df["delay_sec"].rolling(20).mean()
plt.plot(df["window_avg"], label="Rolling MTTR-A (20-run)")
plt.axhline(df.delay_sec.median(), color='r', ls='--', label='Median')
plt.title("Rolling MTTR-A across 200 Runs")
plt.xlabel("Run #"); plt.ylabel("Seconds"); plt.legend(); plt.grid(alpha=.3)
plt.show()


sns.countplot(data=df[df.recovery_mode!="no-drift"], x="recovery_mode", palette="Set2")
plt.title("Frequency of Recovery Modes")
plt.ylabel("Count"); plt.grid(axis="y", alpha=.3)
plt.show()

modes = df[df.delay_sec > 0].groupby("recovery_mode")[["T_detect","T_decide","T_execute"]].mean()
modes.plot(kind="bar", stacked=True, color=["skyblue","orange","green"])
plt.title("Average Latency Breakdown per Recovery Mode")
plt.ylabel("Seconds")
plt.show()

